# =================================
# Agent Trust Demo - Environment Configuration
# =================================
# Copy this file to .env and fill in actual values
# Never commit .env to version control!

# ---------------------------------
# Required: API & Database
# ---------------------------------

# OpenAI API key for LLM-assisted rule generation
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-your-openai-api-key-here

# Supabase project URL
# Format: https://[PROJECT_ID].supabase.co
SUPABASE_URL=https://your-project-id.supabase.co

# Supabase anonymous key (public, read-only access)
SUPABASE_ANON_KEY=your-supabase-anon-key-here

# Supabase service role key (full access, KEEP SECRET!)
# Used for server-side operations, audit logging, rule storage
SUPABASE_SERVICE_ROLE_KEY=your-supabase-service-role-key-here

# ---------------------------------
# Optional: Server Configuration
# ---------------------------------

# Node environment (development, production, test)
NODE_ENV=development

# Server port (default: 3000)
PORT=3000

# ---------------------------------
# Governance & Safety Flags
# ---------------------------------

# Require separate approver for rule changes (two-person rule)
# When true: user who suggests rule cannot approve it
# Default: false (disabled for development)
REQUIRE_SEPARATE_APPROVER=false

# Enable audit logging for all rule operations
# Logs: suggest, apply, reject, dry-run results
# Default: true (always log in production)
ENABLE_AUDIT_LOGGING=true

# Maximum rules per ruleset (prevents performance degradation)
# Default: 100
MAX_RULES_PER_RULESET=100

# Maximum conditions per rule (policy enforcement)
# Must match featureCatalog.json policy.max_conditions_per_rule
# Default: 10
MAX_CONDITIONS_PER_RULE=10

# Dry-run timeout in seconds (prevents long-running queries)
# If dry-run exceeds this, it's killed and marked as timeout
# Default: 5 (5 seconds)
DRY_RUN_TIMEOUT_SECONDS=5

# Dry-run sample size (max transactions to analyze)
# Larger = more accurate, but slower
# Default: 50000
DRY_RUN_SAMPLE_SIZE=50000

# Enable policy gate strict mode (block on warnings, not just errors)
# When true: warnings from policy gate become blocking errors
# Default: false (warnings are non-blocking)
POLICY_GATE_STRICT_MODE=false

# ---------------------------------
# LLM Configuration
# ---------------------------------

# OpenAI model for rule generation
# Options: gpt-4, gpt-4-turbo, gpt-3.5-turbo
# Default: gpt-4
OPENAI_MODEL=gpt-4

# LLM temperature (0-2, lower = more deterministic)
# Default: 0.3 (slightly creative, but mostly deterministic)
LLM_TEMPERATURE=0.3

# LLM max tokens per response
# Default: 2000
LLM_MAX_TOKENS=2000

# LLM request timeout in milliseconds
# Default: 30000 (30 seconds)
LLM_TIMEOUT_MS=30000

# ---------------------------------
# Rate Limiting (Future Sprint 2+)
# ---------------------------------

# Enable rate limiting for /api/rules/* endpoints
# ENABLE_RATE_LIMITING=true

# Max requests per user per hour
# RATE_LIMIT_PER_HOUR=100

# ---------------------------------
# Feature Flags (Future Sprint 2+)
# ---------------------------------

# Enable impact analyzer (dry-run on historical transactions)
# ENABLE_IMPACT_ANALYZER=true

# Enable overlap detection (compare with existing rules)
# ENABLE_OVERLAP_DETECTION=true

# Enable automatic PII stripping in dry-run examples
# ENABLE_PII_STRIPPING=true

# ---------------------------------
# Testing (Used by tests/contract/*.test.js)
# ---------------------------------

# When running tests, set these to placeholder values
# Tests should NOT require real API keys or DB access
# Unit/fuzz/perf tests will use these placeholders

# test:unit, test:fuzz, test:perf → no real API calls
# test:contract → mocked responses (Sprint 2)
# test:golden → deterministic dataset only
